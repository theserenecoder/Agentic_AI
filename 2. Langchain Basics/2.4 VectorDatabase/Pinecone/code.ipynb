{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba1ab7a",
   "metadata": {},
   "source": [
    "### PineCone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06ec7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['PINECONE_API_KEY']=os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cd2dd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings_go = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f0268d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    doc = embeddings_go.embed_query(\"Hello world\")\n",
    "    print(len(doc))\n",
    "except Exception as e:\n",
    "    print(f\"Error embedding query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ef583e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26c9d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'langchain-agentic-ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d7245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if the index exists\n",
    "pc.has_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f32b6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serverless means it will be managed by the cloud provider\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "## Create the index if it does not exist\n",
    "index_name = 'langchain-agentic-ai'\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension = 768,\n",
    "        metric='cosine',\n",
    "        spec = ServerlessSpec(cloud='aws',region='us-east-1')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9c8afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8520e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "## Creating Pinecone Vector Index\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "163b10b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2e61b34d-3b2f-4cd3-a963-f7f1e37efe69',\n",
       " '78bdc4c1-a363-4960-bca8-469e0857dcc1',\n",
       " '05d124ab-8e2e-4405-ba94-a8473c8ca8f5',\n",
       " '6cf267e5-0f9e-4534-8a22-f2d939e2b7fc',\n",
       " 'c4a32634-25a3-40fa-9469-195c5725e18e',\n",
       " '852e5419-712f-4a04-9d27-e30e226b770f',\n",
       " '1058036a-e2b9-4936-86fa-bc0fb8169b28',\n",
       " 'ceedcfcb-e714-4f5c-ad96-788f8555761d',\n",
       " '78b9db2f-12d0-469f-b7a8-8e8f5380767b',\n",
       " 'b6b0d6e4-8fec-4403-b3f3-713ec4199cb8']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## uuid4 is used to generate unique identifiers for the documents\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "\n",
    "## Generating unique identifiers for each document\n",
    "## This is important for FAISS to keep track of the documents.\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents,ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f90f5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out!-[{'source': 'tweet'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications!-[{'source': 'tweet'}]\n",
      "* Wow! That was an amazing movie. I can't wait to see it again.-[{'source': 'tweet'}]\n",
      "* I had chocolate chip pancakes and scrambled eggs for breakfast this morning.-[{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    'what langchain provides to us?',\n",
    "    k=4,\n",
    "    filter={'source':'tweet'}\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f'* {res.page_content}-[{res.metadata}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74a615a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*[SIM:0.692204058] Building an exciting new project with LangChain - come check it out!-[{'source': 'tweet'}]\n",
      "*[SIM:0.664477229] LangGraph is the best framework for building stateful, agentic applications!-[{'source': 'tweet'}]\n",
      "*[SIM:0.492769241] Wow! That was an amazing movie. I can't wait to see it again.-[{'source': 'tweet'}]\n",
      "*[SIM:0.491890401] I had chocolate chip pancakes and scrambled eggs for breakfast this morning.-[{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "results_score = vector_store.similarity_search_with_score(\n",
    "    'what langchain provides to us?',\n",
    "    k=4,\n",
    "    filter = {'source':'tweet'}\n",
    ")\n",
    "\n",
    "for res,scr in results_score:\n",
    "    print(f'*[SIM:{scr}] {res.page_content}-[{res.metadata}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee9ff35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity_score_threshold',\n",
    "    search_kwargs = {'k':1, 'score_threshold':0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "afc7caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ceedcfcb-e714-4f5c-ad96-788f8555761d', metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"LangChain provides abstractions to make working with LLMs easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c02644d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d7cd9",
   "metadata": {},
   "source": [
    "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b73f1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3a42222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question':RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a813f678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It will not be hot tomorrow, as the forecast predicts a high of 62 degrees. The weather is expected to be cloudy and overcast.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('will it be hot tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a9fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
