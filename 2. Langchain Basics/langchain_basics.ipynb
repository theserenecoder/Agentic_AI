{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7dbf82",
   "metadata": {},
   "source": [
    "### Langchain Basics\n",
    "\n",
    "In this section, we'll dive into the fundamental concepts of interacting with Large Language Models (LLMs) using Langchain. We'll explore how to initiate conversations and build powerful applications by understanding the core components:\n",
    "\n",
    "- **ChatOpenAI**: Connecting to OpenAI's smart chat models.\n",
    "- **ChatGroq**: Leveraging Groq's high-speed inference for rapid LLM interactions.\n",
    "- **PromptTemplate**: How to connect to OpenAI's smart chat models..\n",
    "- **Chaining**: Combining multiple steps to create complex LLM workflows.\n",
    "- **StrOutputParser**: Extracting and structuring responses from LLMs.\n",
    "\n",
    "To begin, the first crucial step is to securely load our API keys into the environment.\n",
    "\n",
    "#### Why API Keys Are Loaded in the Environment\n",
    "\n",
    "We put API keys in the environment for two main reasons: security and ease of use. If you put your API keys directly in your code, like writing them down in the program itself, it's a big risk. If someone sees your code (especially if you share it online, like on GitHub), they could see and use your keys.\n",
    "\n",
    "By loading them as environment variables, your keys stay separate from your code. This makes it much harder for them to be seen by accident. Plus, it makes it simple to switch between different project setups (like when you're just building it versus when it's live). Each setup can have its own keys without you ever changing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66bf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## OpenAI and Groq env keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langchain env keys\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea6a8a",
   "metadata": {},
   "source": [
    "##### ChatOpenAI\n",
    "\n",
    "ChatOpenAI is your main way to connect with OpenAI's powerful chat models, such as GPT-3.5 and GPT-4. It makes talking to their service simple, letting you send your questions and get back smart answers. If you're building something that needs to understand and create human-like text, ChatOpenAI is a key tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d9540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Ashu! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "## Initialize the LLM with the desired model\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "## Invoke the LLM with a sample input\n",
    "response = llm.invoke(\"Hi my name is Ashu\")\n",
    "## Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672c4df",
   "metadata": {},
   "source": [
    "##### ChatGroq\n",
    "\n",
    "When you need answers from LLMs super quickly, ChatGroq is a great choice. It's built to use Groq's special computer chips (LPUs), which are known for being incredibly fast. If your program needs answers almost instantly, ChatGroq helps you get quick responses and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fdb828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user said, \"Hi do you know where we are.\" Let me think about how to respond.\n",
      "\n",
      "First, I need to figure out what they're asking. They're probably asking if I know the location we're in. But since I'm an AI, I don't have a physical presence or location. So I should explain that.\n",
      "\n",
      "Wait, maybe they're referring to the digital space or the platform we're on? Like, are they asking about the website or app? But I don't have access to that information either. The user might not be aware that I can't see where they are.\n",
      "\n",
      "Hmm, maybe they're confused or just curious. I should clarify that I don't have a physical location and can't determine their current location. But I can help them if they need information about a place or want to share their location for some reason.\n",
      "\n",
      "I should make sure my response is friendly and helpful, inviting them to ask for specific help. Let me put that together in a clear way without being too technical.\n",
      "</think>\n",
      "\n",
      "Hello! Iâ€™m Qwen, an AI assistant. I donâ€™t have a physical location or awareness of where we are in the real world. However, I can help you explore ideas, answer questions, or even discuss places if youâ€™d like! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "## Initialize the Groq LLM with the desired model\n",
    "llm2 = ChatGroq(model='qwen-qwq-32b')\n",
    "## Invoke the Groq LLM with a sample input\n",
    "response = llm2.invoke(\"Hi do you know where we are\")\n",
    "## Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8763e",
   "metadata": {},
   "source": [
    "##### PromptTemplate\n",
    "\n",
    "A PromptTemplate helps you set up clear instructions for your chat-based LLMs. Instead of just sending messy text, a template lets you define how your messages should look. You can tell the LLM its role (like \"you are a helpful assistant\"), add your questions, and even give it examples. This structure helps the LLM understand exactly what you want, leading to better and more accurate replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727dfd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert in AI Engineer. Provide me an answer based on question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert in AI Engineer. Provide me an answer based on question\"),\n",
    "        ('user','{input}')\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fefced2",
   "metadata": {},
   "source": [
    "##### Chaining\n",
    "\n",
    "Chaining is a powerful way to link up different parts of your LLM application to do more complex jobs. Sometimes, one simple LLM command isn't enough. Chaining lets you connect things like your PromptTemplate, your LLM (from ChatOpenAI or ChatGroq), and your StrOutputParser in a sequence. By breaking down big tasks into smaller steps, chaining helps you build smarter and more reliable AI programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f4a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert in AI Engineer. Provide me an answer based on question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000298A6E358E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000298A6DBE810>, model_name='qwen-qwq-32b', model_kwargs={}, groq_api_key=SecretStr('**********')) \n",
      "\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking, \"What is LangChain?\" Let me break this down. First, I need to recall what I know about LangChain. From what I remember, LangChain is a framework related to AI, specifically for developing applications that use large language models. But I should make sure I get the details right.\n",
      "\n",
      "I think LangChain is designed to help engineers build apps that integrate LLMs. The user might be looking for a basic explanation, so I should start with a simple definition. Then, maybe outline its key features. Let me remember the main components. Oh right, there's prompts, models, chains, agents, and memory. Each of these plays a role in how LangChain structures applications.\n",
      "\n",
      "Wait, the user might not know what LLMs are, so I should briefly explain that. LLM stands for Large Language Models like GPT, which are the backbone of many AI applications. LangChain's purpose is to make working with these models easier. It abstracts away some of the complexity, providing tools to structure the flow of data and logic between user inputs and model outputs.\n",
      "\n",
      "I should mention some key features in detail. For example, modular components allow developers to build complex workflows by combining different parts. Chains are sequences of steps, like taking input, processing with a model, and outputting results. Composability is important because it lets you combine these chains for more complex tasks.\n",
      "\n",
      "Agents in LangChain are probably the part that lets the system decide which tools or steps to take, maybe like a decision-making layer. Tools in LangChain could be external APIs or custom functions, which are integrated into the workflow. Memory is another aspect, allowing the system to remember previous interactions, which is key for conversational applications.\n",
      "\n",
      "Use cases are important too. The user might want to know what LangChain is used for. Examples like chatbots, document analysis, or automation tools come to mind. Also, mentioning supported models like GPT, Anthropic, and others shows versatility.\n",
      "\n",
      "I should also touch on the ecosystem, like integrations with vector databases or other libraries, which helps in building scalable solutions. Finally, highlighting benefits like speed of development and abstraction over raw APIs would be useful for someone evaluating whether to use LangChain.\n",
      "\n",
      "Wait, I should make sure not to confuse LangChain with other frameworks. Maybe clarify that it's Python-based and open-source. Also, ensuring that the explanation is clear and not too technical, as the user might be a developer looking to start using it but needs an overview.\n",
      "\n",
      "Let me structure this step by step: start with a definition, then key components, features, use cases, ecosystem, and benefits. That way the answer is comprehensive but easy to follow. Avoid jargon where possible, but still accurate.\n",
      "</think>\n",
      "\n",
      "**LangChain** is an open-source Python framework designed to simplify the process of building applications powered by **Large Language Models (LLMs)** like GPT, Claude, or others. It provides a structured, modular way to create, test, and deploy AI-driven applications that integrate LLMs into workflows, tools, and real-world use cases.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features & Concepts**\n",
      "1. **Modular Architecture**  \n",
      "   LangChain breaks down common AI workflows into reusable components, allowing developers to:\n",
      "   - **Chains**: Compose sequences of operations (e.g., input â†’ prompt â†’ LLM response â†’ output).\n",
      "   - **Agents**: Build systems that decide *which actions* (e.g., tools or LLM calls) to take based on user input.\n",
      "   - **Tools**: Integrate external APIs, databases, or custom functions into workflows.\n",
      "   - **Memory**: Enable context-awareness by retaining conversation history or state between interactions.\n",
      "\n",
      "2. **Prompt Engineering Utilities**  \n",
      "   Tools to design and optimize prompts for LLMs, including:\n",
      "   - **Prompt Templates**: Predefined structures to standardize input for LLMs.\n",
      "   - **Dynamic Prompts**: Allow parameters and variables to customize outputs.\n",
      "\n",
      "3. **Integration with LLMs and Tools**  \n",
      "   Works with popular LLMs (e.g., OpenAI, Anthropic, Hugging Face) and external tools like:\n",
      "   - Databases (e.g., SQL, vector databases for semantic search).\n",
      "   - APIs (e.g., Google Search, Slack).\n",
      "   - Custom logic (e.g., business rules).\n",
      "\n",
      "4. **Scalability and Efficiency**  \n",
      "   Supports caching, cost management, and parallelization to optimize LLM usage.\n",
      "\n",
      "---\n",
      "\n",
      "### **Common Use Cases**\n",
      "- **Chatbots/Conversational Agents**: Create chatbots with memory and tool access (e.g., a customer support bot that checks internal databases).\n",
      "- **Document Analysis**: Summarize, query, or generate insights from documents using LLMs and vector databases.\n",
      "- **Automation**: Automate repetitive tasks (e.g., generating code, drafting emails, or scheduling tasks based on user input).\n",
      "- **Custom Workflows**: Combine LLM outputs with databases, APIs, and other tools (e.g., \"Find the best flight based on my budget and preferences\").\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Components**\n",
      "- **Prompts**: Define how input is formatted for LLMs.  \n",
      "  Example: `Template(\"Summarize this document: {text}\")`\n",
      "- **Models**: Wrappers for interacting with LLMs (e.g., OpenAI's GPT-4).\n",
      "- **Chains**: Sequences of steps (e.g., `Prompt + LLM + Post-processing`).\n",
      "- **Agents**: Systems that choose which tools/LLMs to use for a task (e.g., the **ZeroShotAgent**, which selects tools based on user instructions).\n",
      "- **Tools**: External functions (e.g., a Google Search API wrapper).\n",
      "- **Memory**: Stores conversation or task history (e.g., `ConversationBufferMemory`).\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Workflow**\n",
      "1. A user asks, \"Find me flights from NYC to SF and book the cheapest one.\"\n",
      "2. The **Agent** decides to first use a **tool** (e.g., a flight API) to search for options.\n",
      "3. The **LLM** generates a summary of the results.\n",
      "4. The **Agent** then uses another **tool** (a booking API) to finalize the booking.\n",
      "5. **Memory** retains the interaction to provide context for follow-up questions.\n",
      "\n",
      "---\n",
      "\n",
      "### **Why Use LangChain?**\n",
      "- **Rapid Prototyping**: Quickly test ideas without reinventing core logic.\n",
      "- **Abstraction**: Handles boilerplate code (e.g., API calls, error handling).\n",
      "- **Flexibility**: Combine LLMs with existing tools and data sources.\n",
      "- **Community & Ecosystem**: Extensive documentation, plugins, and integrations (e.g., support for Chroma DB, FAISS).\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Code Snippet**\n",
      "```python\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import Tool\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# Define a tool (e.g., a function to add numbers)\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "# Create an agent with the tool\n",
      "tools = [Tool(name=\"Addition Tool\", func=add, description=\"Use this for adding numbers\")]\n",
      "agent = initialize_agent(tools, OpenAI(), agent=\"zero-shot-react\", verbose=True)\n",
      "\n",
      "# Ask the agent to perform a task\n",
      "response = agent.run(\"What is 4 + 5?\")\n",
      "print(response)  # Output: 9\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "LangChain is ideal for developers building AI applications that require **contextual understanding**, **tool integration**, and **modular workflows**. It lowers the barrier to entry for leveraging LLMs in real-world scenarios while promoting scalability and maintainability.\n"
     ]
    }
   ],
   "source": [
    "## Chaining\n",
    "chain = prompt|llm2\n",
    "print(chain,'\\n')\n",
    "response = chain.invoke({\"input\":\"What is langchain\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2cb7f",
   "metadata": {},
   "source": [
    "#### Output Parser\n",
    "\n",
    "An output parser is a vital component that takes the raw, unstructured text output from an LLM and transforms it into a more structured and usable format. LLMs often produce free-form text, which can be challenging for downstream applications to process directly. An output parser is responsible for extracting specific information, validating the format (e.g., ensuring it's a JSON object or a specific data type), and converting it into a programmatic data structure like a dictionary or a custom object. This makes the LLM's output readily consumable by other parts of the application and helps maintain data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d105e56",
   "metadata": {},
   "source": [
    "##### StrOutputParser\n",
    "\n",
    "The StrOutputParser is super important for taking the raw text an LLM gives you and turning it into something useful and easy for your program to understand. LLMs often just give back plain text, which can be hard to use directly. A StrOutputParser helps you pull out specific information, check its format, and turn it into a clear string or other data type. This makes the LLM's replies simple to use in the rest of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f35b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To start learning LangChain, a framework designed for developing applications powered by language models, you can follow these steps:\n",
      "\n",
      "1. **Understand the Basics**:\n",
      "   - Get familiar with the fundamentals of language models and their applications. \n",
      "   - It may help to review concepts like NLP (Natural Language Processing), tokenization, and the architecture of models like GPT.\n",
      "\n",
      "2. **Official Documentation**:\n",
      "   - Visit the [LangChain official documentation](https://langchain.readthedocs.io/en/latest/). This is a great resource for understanding the framework's structure, features, and installation process.\n",
      "\n",
      "3. **Set Up Your Environment**:\n",
      "   - Make sure you have Python installed (preferably Python 3.7 or later).\n",
      "   - Create a virtual environment for your project to avoid dependency conflicts:\n",
      "     ```bash\n",
      "     python -m venv langchain-env\n",
      "     source langchain-env/bin/activate  # On Windows, use `langchain-env\\Scripts\\activate`\n",
      "     ```\n",
      "   - Install LangChain using pip:\n",
      "     ```bash\n",
      "     pip install langchain\n",
      "     ```\n",
      "\n",
      "4. **Explore Tutorials and Examples**:\n",
      "   - The documentation often includes tutorials and example projects that demonstrate how to build applications with LangChain.\n",
      "   - Look for beginner projects like chatbots, simple Q&A systems, or text analysis tools to understand how to implement LangChain features.\n",
      "\n",
      "5. **Join Online Communities**:\n",
      "   - Engage with the LangChain community through forums, discussion boards, or social media platforms like Discord or GitHub. This can provide you with support and insights from other learners and experts.\n",
      "\n",
      "6. **Build Simple Projects**:\n",
      "   - Start by creating small-scale projects. For example, build a text summarizer, chatbot, or language translation tool using LangChain.\n",
      "   - Experiment with various modules offered by LangChain, such as chains, memory, and agents.\n",
      "\n",
      "7. **Expand Your Knowledge**:\n",
      "   - As you become more comfortable with LangChain, explore advanced topics such as agent development, custom memory techniques, and integrating APIs.\n",
      "   - Read research papers or articles on recent advancements in LLMs (Large Language Models) and adjacent technologies.\n",
      "\n",
      "8. **Keep Up with Updates**:\n",
      "   - LangChain is continuously evolving. Keep an eye on new features, updates, and best practices by following their GitHub repository or subscribing to newsletters related to AI and NLP.\n",
      "\n",
      "9. **Practice Regularly**:\n",
      "   - The key to mastering any framework or tool is consistent practice. Regularly building projects and experimenting with new ideas can significantly enhance your understanding of LangChain.\n",
      "\n",
      "By following these steps, you can establish a strong foundation in LangChain and leverage its capabilities for various NLP applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm|parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"How to start learning langchain\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36863b09",
   "metadata": {},
   "source": [
    "Using Prompt Template I will set the instructions for LLM to return the result in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e624113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an enpert AI Engineer. Provide the response in json. Provide the answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Engineering with Groq\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_json = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an enpert AI Engineer. Provide the response in json. Provide the answer based on the question\"),\n",
    "        (\"user\",'{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4b451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"joke\": \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chain_json = prompt_json | llm | parser\n",
    "response = chain_json.invoke(\"Tell me a joke\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae80315",
   "metadata": {},
   "source": [
    "Though we were able to ger the result in JSON format, we should the JSONOutputParser with the hwlp of which we can control the structure of our desired output.\n",
    "\n",
    "##### JsonOutputParser\n",
    "\n",
    "It provides the output in a JSON Structured format.It takes the LLM's text output and tries to turn it directly into a JSON object (a dictionary in Python). This is super useful when your program needs to work with structured data, like a list of items or specific details that should be organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea08b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## with Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.6)\n",
    "\n",
    "## Defining our desired data structure\n",
    "class Joke(BaseModel):\n",
    "    question:str = Field(description='question to setup the joke')\n",
    "    punchline: str = Field(description='answer to resolve the question')\n",
    "    \n",
    "## Query intended to prompt LLM to populate the data structure\n",
    "joke_query = 'Tell me a joke'\n",
    "\n",
    "## Setup a parser and inject the instructions into the prompt template    \n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables = ['query'],\n",
    "    partial_variables={'format_instructions':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "response = chain.invoke({'query':joke_query})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
